"""
Syst√®me d'√©valuation pour le module VLM de FacturAI
"""

import os
import json
import time
import logging
from typing import Dict, List, Any, Tuple, Optional
from pathlib import Path
import statistics
import matplotlib.pyplot as plt
import pandas as pd

from .vlm_processor import VLMProcessor
from .utils import VLMVisualizer

logger = logging.getLogger(__name__)

class VLMEvaluator:
    """
    Syst√®me d'√©valuation pour le module VLM
    
    √âvalue les performances de d√©tection de zones, qualit√© d'analyse,
    temps de traitement et comparaison entre mod√®les
    """
    
    def __init__(self, output_dir: str = "Data/vlm_evaluation"):
        """
        Initialise l'√©valuateur
        
        Args:
            output_dir: R√©pertoire de sortie pour les √©valuations
        """
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        self.processor = VLMProcessor()
        self.visualizer = VLMVisualizer()
        
        # M√©triques par d√©faut
        self.metrics = {
            'zone_detection': {
                'header_precision': 0.0,
                'header_recall': 0.0,
                'footer_precision': 0.0,
                'footer_recall': 0.0,
                'table_precision': 0.0,
                'table_recall': 0.0,
                'amount_precision': 0.0,
                'amount_recall': 0.0,
                'overall_f1': 0.0
            },
            'performance': {
                'avg_processing_time': 0.0,
                'min_processing_time': 0.0,
                'max_processing_time': 0.0,
                'throughput': 0.0  # images/second
            },
            'quality': {
                'avg_confidence': 0.0,
                'layout_quality_score': 0.0,
                'integration_score': 0.0
            }
        }
    
    def evaluate_dataset(self, images_dir: str, ground_truth_file: str = None, 
                        ocr_results_dir: str = None) -> Dict[str, Any]:
        """
        √âvalue le module VLM sur un dataset complet
        
        Args:
            images_dir: R√©pertoire contenant les images de test
            ground_truth_file: Fichier JSON avec les v√©rit√©s terrain (optionnel)
            ocr_results_dir: R√©pertoire avec les r√©sultats OCR correspondants
        
        Returns:
            M√©triques d'√©valuation compl√®tes
        """
        print(f"üìä √âvaluation du dataset: {images_dir}")
        
        # Collecte des images
        image_files = self._collect_image_files(images_dir)
        if not image_files:
            raise ValueError(f"Aucune image trouv√©e dans {images_dir}")
        
        print(f"üìÅ {len(image_files)} images √† traiter")
        
        # Chargement de la v√©rit√© terrain
        ground_truth = {}
        if ground_truth_file and os.path.exists(ground_truth_file):
            with open(ground_truth_file, 'r', encoding='utf-8') as f:
                ground_truth = json.load(f)
            print(f"‚úÖ V√©rit√© terrain charg√©e: {len(ground_truth)} annotations")
        
        # Traitement et √©valuation
        results = []
        processing_times = []
        
        start_time = time.time()
        
        for i, image_path in enumerate(image_files):
            print(f"üîÑ Traitement {i+1}/{len(image_files)}: {os.path.basename(image_path)}")
            
            try:
                # Chargement des r√©sultats OCR si disponibles
                ocr_results = self._load_ocr_results(image_path, ocr_results_dir)
                
                # Traitement VLM
                result = self.processor.process_invoice(image_path, ocr_results)
                results.append(result)
                
                processing_times.append(result.get('processing_time', 0))
                
                # √âvaluation contre la v√©rit√© terrain
                if ground_truth:
                    image_name = os.path.basename(image_path)
                    if image_name in ground_truth:
                        eval_result = self._evaluate_against_ground_truth(
                            result, ground_truth[image_name]
                        )
                        result['evaluation'] = eval_result
                
            except Exception as e:
                logger.error(f"Erreur lors du traitement de {image_path}: {e}")
                continue
        
        total_time = time.time() - start_time
        
        # Calcul des m√©triques globales
        metrics = self._calculate_global_metrics(results, processing_times, total_time)
        
        # Sauvegarde des r√©sultats
        evaluation_report = {
            'timestamp': time.time(),
            'dataset_info': {
                'images_dir': images_dir,
                'total_images': len(image_files),
                'processed_images': len(results),
                'ground_truth_available': bool(ground_truth),
                'ocr_integration': ocr_results_dir is not None
            },
            'metrics': metrics,
            'detailed_results': results,
            'model_info': self.processor.get_model_info()
        }
        
        # Sauvegarde
        report_file = os.path.join(self.output_dir, f"evaluation_report_{int(time.time())}.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"‚úÖ √âvaluation termin√©e: {report_file}")
        
        # G√©n√©ration de visualisations
        self._generate_evaluation_visualizations(evaluation_report)
        
        return evaluation_report
    
    def compare_models(self, images_dir: str, models: List[str], 
                      ground_truth_file: str = None) -> Dict[str, Any]:
        """
        Compare les performances de diff√©rents mod√®les VLM
        
        Args:
            images_dir: R√©pertoire d'images de test
            models: Liste des mod√®les √† comparer
            ground_truth_file: Fichier de v√©rit√© terrain
        
        Returns:
            Comparaison d√©taill√©e des mod√®les
        """
        print(f"üî¨ Comparaison de {len(models)} mod√®les VLM")
        
        comparison_results = {
            'timestamp': time.time(),
            'models_compared': models,
            'results_by_model': {},
            'comparison_metrics': {}
        }
        
        image_files = self._collect_image_files(images_dir)
        
        for model_name in models:
            print(f"\nüìà √âvaluation du mod√®le: {model_name}")
            
            try:
                # Chargement du mod√®le
                self.processor.load_model(model_name)
                
                # √âvaluation sur le dataset
                model_results = self.evaluate_dataset(images_dir, ground_truth_file)
                comparison_results['results_by_model'][model_name] = model_results
                
            except Exception as e:
                logger.error(f"Erreur avec le mod√®le {model_name}: {e}")
                comparison_results['results_by_model'][model_name] = {'error': str(e)}
        
        # Calcul des m√©triques de comparaison
        comparison_results['comparison_metrics'] = self._calculate_comparison_metrics(
            comparison_results['results_by_model']
        )
        
        # Sauvegarde
        comparison_file = os.path.join(self.output_dir, f"model_comparison_{int(time.time())}.json")
        with open(comparison_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_results, f, indent=2, ensure_ascii=False, default=str)
        
        # G√©n√©ration de graphiques comparatifs
        self._generate_comparison_charts(comparison_results)
        
        print(f"‚úÖ Comparaison termin√©e: {comparison_file}")
        return comparison_results
    
    def benchmark_performance(self, images_dir: str, iterations: int = 3) -> Dict[str, Any]:
        """
        Effectue un benchmark de performance
        
        Args:
            images_dir: R√©pertoire d'images de test
            iterations: Nombre d'it√©rations pour la moyenne
        
        Returns:
            R√©sultats de benchmark
        """
        print(f"‚ö° Benchmark de performance ({iterations} it√©rations)")
        
        image_files = self._collect_image_files(images_dir)
        if len(image_files) > 10:
            image_files = image_files[:10]  # Limiter pour le benchmark
        
        benchmark_results = {
            'timestamp': time.time(),
            'iterations': iterations,
            'images_count': len(image_files),
            'model_info': self.processor.get_model_info(),
            'performance_data': []
        }
        
        for iteration in range(iterations):
            print(f"üîÑ It√©ration {iteration + 1}/{iterations}")
            
            iteration_times = []
            iteration_start = time.time()
            
            for image_path in image_files:
                start_time = time.time()
                try:
                    result = self.processor.process_invoice(image_path)
                    processing_time = time.time() - start_time
                    iteration_times.append(processing_time)
                except Exception as e:
                    logger.error(f"Erreur: {e}")
                    continue
            
            iteration_total = time.time() - iteration_start
            
            iteration_data = {
                'iteration': iteration + 1,
                'total_time': iteration_total,
                'avg_time_per_image': statistics.mean(iteration_times) if iteration_times else 0,
                'min_time': min(iteration_times) if iteration_times else 0,
                'max_time': max(iteration_times) if iteration_times else 0,
                'throughput': len(iteration_times) / iteration_total if iteration_total > 0 else 0,
                'individual_times': iteration_times
            }
            
            benchmark_results['performance_data'].append(iteration_data)
        
        # Calcul des moyennes
        avg_metrics = self._calculate_benchmark_averages(benchmark_results['performance_data'])
        benchmark_results['average_metrics'] = avg_metrics
        
        # Sauvegarde
        benchmark_file = os.path.join(self.output_dir, f"benchmark_{int(time.time())}.json")
        with open(benchmark_file, 'w', encoding='utf-8') as f:
            json.dump(benchmark_results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"‚úÖ Benchmark termin√©: {benchmark_file}")
        print(f"üìä D√©bit moyen: {avg_metrics['avg_throughput']:.2f} images/sec")
        print(f"‚è±Ô∏è  Temps moyen: {avg_metrics['avg_time_per_image']:.2f}s/image")
        
        return benchmark_results
    
    def _collect_image_files(self, images_dir: str) -> List[str]:
        """Collecte les fichiers images dans un r√©pertoire"""
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
        image_files = []
        
        for ext in image_extensions:
            image_files.extend(Path(images_dir).glob(f"*{ext}"))
            image_files.extend(Path(images_dir).glob(f"*{ext.upper()}"))
        
        return [str(f) for f in sorted(image_files)]
    
    def _load_ocr_results(self, image_path: str, ocr_results_dir: str) -> Optional[Dict]:
        """Charge les r√©sultats OCR correspondant √† une image"""
        if not ocr_results_dir:
            return None
        
        image_name = Path(image_path).stem
        ocr_patterns = [
            f"enhanced_{image_name}_ocr.json",
            f"{image_name}_ocr.json",
            f"ocr_{image_name}.json"
        ]
        
        for pattern in ocr_patterns:
            ocr_file = os.path.join(ocr_results_dir, pattern)
            if os.path.exists(ocr_file):
                try:
                    with open(ocr_file, 'r', encoding='utf-8') as f:
                        return json.load(f)
                except Exception as e:
                    logger.error(f"Erreur lors du chargement OCR {ocr_file}: {e}")
        
        return None
    
    def _evaluate_against_ground_truth(self, result: Dict[str, Any], 
                                     ground_truth: Dict[str, Any]) -> Dict[str, Any]:
        """√âvalue un r√©sultat contre la v√©rit√© terrain"""
        evaluation = {
            'zone_detection': {
                'header': self._evaluate_zone_detection(
                    result.get('detected_zones', {}).get('header', {}),
                    ground_truth.get('header', {})
                ),
                'footer': self._evaluate_zone_detection(
                    result.get('detected_zones', {}).get('footer', {}),
                    ground_truth.get('footer', {})
                ),
                'tables': self._evaluate_tables_detection(
                    result.get('detected_zones', {}).get('tables', []),
                    ground_truth.get('tables', [])
                ),
                'amounts': self._evaluate_amounts_detection(
                    result.get('detected_zones', {}).get('amount_zones', []),
                    ground_truth.get('amounts', [])
                )
            },
            'layout_quality': self._evaluate_layout_quality(result, ground_truth)
        }
        
        return evaluation
    
    def _evaluate_zone_detection(self, detected: Dict[str, Any], 
                                expected: Dict[str, Any]) -> Dict[str, float]:
        """√âvalue la d√©tection d'une zone sp√©cifique"""
        detected_present = detected.get('detected', False)
        expected_present = expected.get('present', False)
        
        # Calcul pr√©cision/rappel
        if detected_present and expected_present:
            precision = recall = 1.0  # True Positive
        elif detected_present and not expected_present:
            precision = 0.0  # False Positive
            recall = 0.0
        elif not detected_present and expected_present:
            precision = 0.0
            recall = 0.0  # False Negative
        else:
            precision = recall = 1.0  # True Negative
        
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        return {
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score,
            'confidence': detected.get('confidence', 0)
        }
    
    def _evaluate_tables_detection(self, detected: List[Dict], 
                                  expected: List[Dict]) -> Dict[str, float]:
        """√âvalue la d√©tection de tableaux"""
        detected_count = len(detected)
        expected_count = len(expected)
        
        if expected_count == 0:
            precision = 1.0 if detected_count == 0 else 0.0
            recall = 1.0
        else:
            # Approximation simple
            true_positives = min(detected_count, expected_count)
            precision = true_positives / detected_count if detected_count > 0 else 0
            recall = true_positives / expected_count
        
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        return {
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score,
            'detected_count': detected_count,
            'expected_count': expected_count
        }
    
    def _evaluate_amounts_detection(self, detected: List[Dict], 
                                   expected: List[Dict]) -> Dict[str, float]:
        """√âvalue la d√©tection de montants"""
        # √âvaluation bas√©e sur la correspondance des valeurs
        detected_values = [float(str(a.get('value', 0)).replace(',', '.')) 
                          for a in detected if a.get('value')]
        expected_values = [float(str(a.get('value', 0)).replace(',', '.')) 
                          for a in expected if a.get('value')]
        
        matches = 0
        for expected_val in expected_values:
            for detected_val in detected_values:
                if abs(detected_val - expected_val) < 0.01:
                    matches += 1
                    break
        
        precision = matches / len(detected_values) if detected_values else 0
        recall = matches / len(expected_values) if expected_values else 1
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        return {
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score,
            'matches': matches,
            'detected_count': len(detected_values),
            'expected_count': len(expected_values)
        }
    
    def _evaluate_layout_quality(self, result: Dict[str, Any], 
                                ground_truth: Dict[str, Any]) -> float:
        """√âvalue la qualit√© de l'analyse de mise en page"""
        layout_analysis = result.get('layout_analysis', {})
        quality_score = layout_analysis.get('layout_quality', {}).get('overall_score', 0)
        
        # Comparaison avec la v√©rit√© terrain si disponible
        expected_quality = ground_truth.get('layout_quality', 0.8)  # Valeur par d√©faut
        
        # Score bas√© sur la proximit√© avec la qualit√© attendue
        quality_accuracy = 1 - abs(quality_score - expected_quality)
        
        return max(quality_accuracy, 0)
    
    def _calculate_global_metrics(self, results: List[Dict], 
                                 processing_times: List[float], 
                                 total_time: float) -> Dict[str, Any]:
        """Calcule les m√©triques globales"""
        metrics = {
            'zone_detection': {
                'header_precision': 0.0,
                'header_recall': 0.0,
                'footer_precision': 0.0,
                'footer_recall': 0.0,
                'table_f1': 0.0,
                'amount_f1': 0.0,
                'overall_f1': 0.0
            },
            'performance': {
                'avg_processing_time': statistics.mean(processing_times) if processing_times else 0,
                'min_processing_time': min(processing_times) if processing_times else 0,
                'max_processing_time': max(processing_times) if processing_times else 0,
                'total_time': total_time,
                'throughput': len(results) / total_time if total_time > 0 else 0
            },
            'quality': {
                'avg_confidence': 0.0,
                'avg_layout_quality': 0.0,
                'success_rate': len([r for r in results if 'error' not in r]) / len(results) if results else 0
            }
        }
        
        # Calcul des moyennes pour les m√©triques de qualit√©
        confidences = []
        layout_qualities = []
        
        for result in results:
            vlm_analysis = result.get('vlm_analysis', {})
            confidences.append(vlm_analysis.get('confidence', 0))
            
            layout_analysis = result.get('layout_analysis', {})
            layout_quality = layout_analysis.get('layout_quality', {}).get('overall_score', 0)
            layout_qualities.append(layout_quality)
        
        if confidences:
            metrics['quality']['avg_confidence'] = statistics.mean(confidences)
        if layout_qualities:
            metrics['quality']['avg_layout_quality'] = statistics.mean(layout_qualities)
        
        return metrics
    
    def _calculate_comparison_metrics(self, results_by_model: Dict[str, Any]) -> Dict[str, Any]:
        """Calcule les m√©triques de comparaison entre mod√®les"""
        comparison = {
            'best_model': {
                'overall_performance': None,
                'fastest': None,
                'most_accurate': None,
                'best_quality': None
            },
            'rankings': {}
        }
        
        model_scores = {}
        
        for model_name, model_results in results_by_model.items():
            if 'error' in model_results:
                continue
            
            metrics = model_results.get('metrics', {})
            performance = metrics.get('performance', {})
            quality = metrics.get('quality', {})
            
            # Score composite
            composite_score = (
                quality.get('avg_confidence', 0) * 0.3 +
                quality.get('avg_layout_quality', 0) * 0.3 +
                (1 / max(performance.get('avg_processing_time', 1), 0.1)) * 0.2 +
                quality.get('success_rate', 0) * 0.2
            )
            
            model_scores[model_name] = {
                'composite_score': composite_score,
                'avg_time': performance.get('avg_processing_time', float('inf')),
                'confidence': quality.get('avg_confidence', 0),
                'layout_quality': quality.get('avg_layout_quality', 0)
            }
        
        if model_scores:
            # D√©termination des meilleurs mod√®les
            best_overall = max(model_scores.items(), key=lambda x: x[1]['composite_score'])
            fastest = min(model_scores.items(), key=lambda x: x[1]['avg_time'])
            most_accurate = max(model_scores.items(), key=lambda x: x[1]['confidence'])
            best_quality = max(model_scores.items(), key=lambda x: x[1]['layout_quality'])
            
            comparison['best_model'].update({
                'overall_performance': best_overall[0],
                'fastest': fastest[0],
                'most_accurate': most_accurate[0],
                'best_quality': best_quality[0]
            })
            
            # Classements
            comparison['rankings'] = {
                'by_composite_score': sorted(model_scores.items(), 
                                           key=lambda x: x[1]['composite_score'], reverse=True),
                'by_speed': sorted(model_scores.items(), key=lambda x: x[1]['avg_time']),
                'by_accuracy': sorted(model_scores.items(), 
                                    key=lambda x: x[1]['confidence'], reverse=True)
            }
        
        return comparison
    
    def _calculate_benchmark_averages(self, performance_data: List[Dict]) -> Dict[str, float]:
        """Calcule les moyennes de benchmark"""
        if not performance_data:
            return {}
        
        total_times = [d['total_time'] for d in performance_data]
        avg_times = [d['avg_time_per_image'] for d in performance_data]
        throughputs = [d['throughput'] for d in performance_data]
        
        return {
            'avg_total_time': statistics.mean(total_times),
            'avg_time_per_image': statistics.mean(avg_times),
            'avg_throughput': statistics.mean(throughputs),
            'std_time_per_image': statistics.stdev(avg_times) if len(avg_times) > 1 else 0
        }
    
    def _generate_evaluation_visualizations(self, evaluation_report: Dict[str, Any]):
        """G√©n√®re les visualisations d'√©valuation"""
        try:
            metrics = evaluation_report.get('metrics', {})
            
            # Graphique des performances
            plt.figure(figsize=(12, 8))
            
            # Temps de traitement
            plt.subplot(2, 2, 1)
            performance = metrics.get('performance', {})
            times = [performance.get('min_processing_time', 0),
                    performance.get('avg_processing_time', 0),
                    performance.get('max_processing_time', 0)]
            labels = ['Min', 'Avg', 'Max']
            plt.bar(labels, times)
            plt.title('Temps de traitement (secondes)')
            plt.ylabel('Secondes')
            
            # Qualit√©
            plt.subplot(2, 2, 2)
            quality = metrics.get('quality', {})
            quality_metrics = [
                quality.get('avg_confidence', 0),
                quality.get('avg_layout_quality', 0),
                quality.get('success_rate', 0)
            ]
            quality_labels = ['Confiance', 'Qualit√© Layout', 'Taux Succ√®s']
            plt.bar(quality_labels, quality_metrics)
            plt.title('M√©triques de qualit√©')
            plt.ylabel('Score (0-1)')
            plt.xticks(rotation=45)
            
            plt.tight_layout()
            
            # Sauvegarde
            viz_file = os.path.join(self.output_dir, f"evaluation_viz_{int(time.time())}.png")
            plt.savefig(viz_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"üìà Visualisation g√©n√©r√©e: {viz_file}")
            
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration de visualisations: {e}")
    
    def _generate_comparison_charts(self, comparison_results: Dict[str, Any]):
        """G√©n√®re les graphiques de comparaison de mod√®les"""
        try:
            results_by_model = comparison_results.get('results_by_model', {})
            
            if len(results_by_model) < 2:
                return
            
            # Pr√©paration des donn√©es
            models = []
            avg_times = []
            confidences = []
            
            for model_name, model_data in results_by_model.items():
                if 'error' in model_data:
                    continue
                
                models.append(model_name)
                metrics = model_data.get('metrics', {})
                avg_times.append(metrics.get('performance', {}).get('avg_processing_time', 0))
                confidences.append(metrics.get('quality', {}).get('avg_confidence', 0))
            
            if len(models) < 2:
                return
            
            # Graphique de comparaison
            plt.figure(figsize=(14, 6))
            
            # Temps de traitement
            plt.subplot(1, 2, 1)
            plt.bar(models, avg_times)
            plt.title('Temps de traitement moyen par mod√®le')
            plt.ylabel('Secondes')
            plt.xticks(rotation=45)
            
            # Confiance
            plt.subplot(1, 2, 2)
            plt.bar(models, confidences)
            plt.title('Confiance moyenne par mod√®le')
            plt.ylabel('Score de confiance')
            plt.xticks(rotation=45)
            
            plt.tight_layout()
            
            # Sauvegarde
            comp_file = os.path.join(self.output_dir, f"model_comparison_{int(time.time())}.png")
            plt.savefig(comp_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"üìä Comparaison g√©n√©r√©e: {comp_file}")
            
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration de comparaison: {e}")

def main():
    """Exemple d'utilisation du syst√®me d'√©valuation"""
    evaluator = VLMEvaluator()
    
    # √âvaluation simple
    results = evaluator.evaluate_dataset("../Data/processed_images")
    print(f"R√©sultats d'√©valuation: {results['metrics']}")
    
    # Benchmark de performance
    benchmark = evaluator.benchmark_performance("../Data/processed_images", iterations=2)
    print(f"Benchmark: {benchmark['average_metrics']}")

if __name__ == "__main__":
    main()