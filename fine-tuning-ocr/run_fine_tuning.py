#!/usr/bin/env python3
"""
Script principal pour lancer le fine-tuning OCR complet
Interface utilisateur simplifi√©e pour FacturAI
"""

import os
import sys
import json
import logging
import argparse
from pathlib import Path
from typing import Dict, Any, List
import time
from datetime import datetime

# Imports des modules du syst√®me
from data_preparation.data_preparation import InvoiceDataPreparator
from fine_tuning_manager.fine_tuning_manager import OCRFineTuningManager
from evaluation.model_evaluation import OCRModelEvaluator

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FacturAIFineTuningOrchestrator:
    """Orchestrateur principal pour le fine-tuning FacturAI"""
    
    def __init__(self, config_file: str = "fine_tuning_config.json"):
        self.config_file = config_file
        self.config = self.load_or_create_config()
        self.setup_directories()
        
        logger.info("üöÄ FacturAI Fine-Tuning Orchestrator initialis√©")
    
    def load_or_create_config(self) -> Dict[str, Any]:
        """Charge ou cr√©e la configuration"""
        if Path(self.config_file).exists():
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info(f"Configuration charg√©e depuis {self.config_file}")
        else:
            config = self.create_default_config()
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            logger.info(f"Configuration par d√©faut cr√©√©e: {self.config_file}")
        
        return config
    
    def create_default_config(self) -> Dict[str, Any]:
        """Cr√©e la configuration par d√©faut"""
        return {
            "project_name": "FacturAI Fine-Tuning",
            "version": "1.0.0",
            "data": {
                "images_dir": "Data/processed_images",
                "ocr_results_dir": "Data/ocr_results",
                "output_dir": "Data/fine_tuning",
                "train_split": 0.8,
                "val_split": 0.1,
                "test_split": 0.1
            },
            "models": {
                "easyocr": {
                    "enabled": True,
                    "epochs": 50,
                    "batch_size": 8,
                    "learning_rate": 0.001,
                    "output_dir": "fine-tuning-ocr/models/easyocr_finetuned"
                },
                "trocr": {
                    "enabled": True,
                    "base_model": "microsoft/trocr-large-printed",
                    "epochs": 10,
                    "batch_size": 4,
                    "learning_rate": 5e-5,
                    "output_dir": "fine-tuning-ocr/models/trocr_finetuned"
                }
            },
            "evaluation": {
                "output_dir": "evaluation_results",
                "metrics": ["similarity", "confidence", "speed", "accuracy"]
            },
            "hardware": {
                "use_gpu": True,
                "gpu_memory_limit": "8GB"
            }
        }
    
    def setup_directories(self):
        """Cr√©e les dossiers n√©cessaires"""
        directories = [
            self.config["data"]["output_dir"],
            self.config["models"]["easyocr"]["output_dir"],
            self.config["models"]["trocr"]["output_dir"],
            self.config["evaluation"]["output_dir"],
            "logs"
        ]
        
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
    
    def check_requirements(self) -> bool:
        """V√©rifie que tout est pr√™t pour le fine-tuning"""
        logger.info("üîç V√©rification des pr√©requis...")
        
        issues = []
        
        # V√©rifier les dossiers de donn√©es
        images_dir = Path(self.config["data"]["images_dir"])
        ocr_dir = Path(self.config["data"]["ocr_results_dir"])
        
        if not images_dir.exists():
            issues.append(f"Dossier images manquant: {images_dir}")
        elif len(list(images_dir.glob("*.png"))) == 0:
            issues.append(f"Aucune image trouv√©e dans: {images_dir}")
        
        if not ocr_dir.exists():
            issues.append(f"Dossier OCR manquant: {ocr_dir}")
        elif len(list(ocr_dir.glob("*.json"))) == 0:
            issues.append(f"Aucun r√©sultat OCR trouv√© dans: {ocr_dir}")
        
        # V√©rifier les d√©pendances Python
        try:
            import torch
            import transformers
            import easyocr
            logger.info("‚úÖ D√©pendances Python OK")
        except ImportError as e:
            issues.append(f"D√©pendance manquante: {e}")
        
        # V√©rifier GPU si activ√©
        if self.config["hardware"]["use_gpu"]:
            try:
                import torch
                if torch.cuda.is_available():
                    gpu_name = torch.cuda.get_device_name(0)
                    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
                    logger.info(f"‚úÖ GPU disponible: {gpu_name} ({gpu_memory:.1f}GB)")
                else:
                    issues.append("GPU demand√© mais non disponible")
            except:
                issues.append("Impossible de v√©rifier le GPU")
        
        if issues:
            logger.error("‚ùå Probl√®mes d√©tect√©s:")
            for issue in issues:
                logger.error(f"  - {issue}")
            return False
        
        logger.info("‚úÖ Tous les pr√©requis sont satisfaits")
        return True
    
    def prepare_data(self) -> Dict[str, Any]:
        """Pr√©pare les donn√©es pour le fine-tuning"""
        logger.info("üìä √âTAPE 1: Pr√©paration des donn√©es")
        logger.info("=" * 50)
        
        preparator = InvoiceDataPreparator(
            images_dir=self.config["data"]["images_dir"],
            ocr_results_dir=self.config["data"]["ocr_results_dir"],
            output_dir=self.config["data"]["output_dir"]
        )
        
        results = preparator.run_complete_preparation()
        
        if results:
            logger.info("‚úÖ Pr√©paration des donn√©es termin√©e")
            
            # Sauvegarder les statistiques dans la config
            self.config["data"]["statistics"] = results["statistics"]
            self.save_config()
            
            return results
        else:
            logger.error("‚ùå √âchec de la pr√©paration des donn√©es")
            return {}
    
    def train_models(self, enabled_models: List[str] = None) -> Dict[str, Any]:
        """Lance l'entra√Ænement des mod√®les s√©lectionn√©s"""
        logger.info("üéØ √âTAPE 2: Entra√Ænement des mod√®les")
        logger.info("=" * 50)
        
        if enabled_models is None:
            enabled_models = [model for model in self.config["models"] 
                            if self.config["models"][model].get("enabled", False)]
        
        training_results = {}
        
        # EasyOCR
        if "easyocr" in enabled_models:
            logger.info("\nüëÅÔ∏è Entra√Ænement EasyOCR...")
            try:
                from fine_tuning_model.easyocr_finetuning import EasyOCRFineTuner
                
                tuner = EasyOCRFineTuner(self.config["models"]["easyocr"])
                dataset_file = f"{self.config['data']['output_dir']}/datasets/easyocr/dataset.json"
                
                results = tuner.train(
                    dataset_file=dataset_file,
                    epochs=self.config["models"]["easyocr"]["epochs"],
                    batch_size=self.config["models"]["easyocr"]["batch_size"]
                )
                
                training_results["easyocr"] = results
                logger.info("‚úÖ EasyOCR entra√Æn√© avec succ√®s")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur EasyOCR: {e}")
                training_results["easyocr"] = {"error": str(e)}
        
        # TrOCR
        if "trocr" in enabled_models:
            logger.info("\nü§ñ Entra√Ænement TrOCR...")
            try:
                from fine_tuning_model.trocr_finetuning import TrOCRFineTuner
                
                tuner = TrOCRFineTuner(self.config["models"]["trocr"])
                dataset_file = f"{self.config['data']['output_dir']}/datasets/trocr/dataset.json"
                
                results = tuner.train(
                    dataset_file=dataset_file,
                    epochs=self.config["models"]["trocr"]["epochs"],
                    batch_size=self.config["models"]["trocr"]["batch_size"]
                )
                
                training_results["trocr"] = results
                logger.info("‚úÖ TrOCR entra√Æn√© avec succ√®s")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur TrOCR: {e}")
                training_results["trocr"] = {"error": str(e)}
        
        return training_results
    
    def evaluate_models(self, training_results: Dict[str, Any]) -> Dict[str, Any]:
        """√âvalue et compare les mod√®les entra√Æn√©s"""
        logger.info("üìà √âTAPE 3: √âvaluation des mod√®les")
        logger.info("=" * 50)
        
        # Pr√©parer les chemins des mod√®les
        models_config = {}
        
        for model_name, results in training_results.items():
            if "error" not in results:
                if model_name == "easyocr" and "model_path" in results:
                    models_config["easyocr_finetuned"] = results["model_path"]
                elif model_name == "trocr" and "model_path" in results:
                    models_config["trocr_finetuned"] = results["model_path"]
        
        # Lancer l'√©valuation
        evaluator = OCRModelEvaluator(self.config["evaluation"]["output_dir"])
        
        test_data_file = f"{self.config['data']['output_dir']}/splits/test.json"
        ground_truth_file = f"{self.config['data']['output_dir']}/annotations/ground_truth.json"
        
        evaluation_results = evaluator.run_complete_evaluation(
            test_data_file=test_data_file,
            ground_truth_file=ground_truth_file,
            models_config=models_config
        )
        
        return evaluation_results
    
    def generate_final_report(self, preparation_results: Dict, training_results: Dict, 
                            evaluation_results: Dict) -> str:
        """G√©n√®re le rapport final complet"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = Path("logs") / f"facturai_fine_tuning_report_{timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# üéØ Rapport Final - Fine-Tuning FacturAI\n\n")
            f.write(f"**Date :** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**Version :** {self.config['version']}\n\n")
            
            # R√©sum√© ex√©cutif
            f.write("## üìä R√©sum√© Ex√©cutif\n\n")
            
            if preparation_results and "statistics" in preparation_results:
                stats = preparation_results["statistics"]
                f.write(f"- **Images trait√©es :** {stats['total_images']}\n")
                f.write(f"- **Annotations g√©n√©r√©es :** {stats['total_annotations']}\n")
                f.write(f"- **Confiance moyenne initiale :** {stats['avg_confidence']:.2f}\n\n")
            
            # Mod√®les entra√Æn√©s
            f.write("## ü§ñ Mod√®les Entra√Æn√©s\n\n")
            
            for model_name, results in training_results.items():
                f.write(f"### {model_name.upper()}\n")
                if "error" in results:
                    f.write(f"‚ùå **Status :** √âchec - {results['error']}\n\n")
                else:
                    f.write("‚úÖ **Status :** Succ√®s\n")
                    if "total_time" in results:
                        f.write(f"‚è±Ô∏è **Temps d'entra√Ænement :** {results['total_time']/60:.1f} minutes\n")
                    if "best_val_loss" in results:
                        f.write(f"üìâ **Meilleure perte :** {results['best_val_loss']:.4f}\n")
                    f.write("\n")
            
            # R√©sultats d'√©valuation
            if evaluation_results and "comparison_df" in evaluation_results:
                f.write("## üèÜ Classement des Mod√®les\n\n")
                df = evaluation_results["comparison_df"]
                if not df.empty:
                    for i, (_, row) in enumerate(df.iterrows(), 1):
                        f.write(f"{i}. **{row['Model']}**\n")
                        if 'avg_similarity' in row:
                            f.write(f"   - Similarit√© : {row['avg_similarity']:.3f}\n")
                        if 'avg_confidence' in row:
                            f.write(f"   - Confiance : {row['avg_confidence']:.3f}\n")
                        if 'avg_processing_time' in row:
                            f.write(f"   - Vitesse : {row['avg_processing_time']:.2f}s\n")
                        f.write("\n")
            
            # Recommandations
            f.write("## üí° Recommandations\n\n")
            f.write("### Pour la Production\n")
            f.write("1. **Mod√®le recommand√© :** Le mod√®le avec la meilleure similarit√©\n")
            f.write("2. **Optimisations :**\n")
            f.write("   - Pr√©processing adapt√© aux factures\n")
            f.write("   - Post-processing avec validation m√©tier\n")
            f.write("   - Syst√®me de confiance pour la validation manuelle\n\n")
            
            f.write("### Prochaines √âtapes\n")
            f.write("1. Tests sur de nouvelles factures\n")
            f.write("2. Int√©gration dans le pipeline FacturAI\n")
            f.write("3. Monitoring des performances en production\n")
            f.write("4. Am√©lioration continue avec nouvelles donn√©es\n\n")
            
            # Fichiers g√©n√©r√©s
            f.write("## üìÅ Fichiers G√©n√©r√©s\n\n")
            f.write(f"- **Configuration :** `{self.config_file}`\n")
            f.write(f"- **Donn√©es :** `{self.config['data']['output_dir']}/`\n")
            f.write(f"- **Mod√®les :** `models/`\n")
            f.write(f"- **√âvaluation :** `{self.config['evaluation']['output_dir']}/`\n")
            
            if evaluation_results and "report_file" in evaluation_results:
                f.write(f"- **Rapport d√©taill√© :** `{evaluation_results['report_file']}`\n")
        
        return str(report_file)
    
    def save_config(self):
        """Sauvegarde la configuration mise √† jour"""
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, indent=2, ensure_ascii=False)
    
    def run_complete_pipeline(self, enabled_models: List[str] = None) -> Dict[str, Any]:
        """Lance le pipeline complet de fine-tuning"""
        logger.info("üöÄ D√âMARRAGE DU PIPELINE COMPLET FACTURAI")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        # V√©rifications
        if not self.check_requirements():
            return {"error": "Pr√©requis non satisfaits"}
        
        try:
            # 1. Pr√©paration des donn√©es
            preparation_results = self.prepare_data()
            if not preparation_results:
                return {"error": "√âchec de la pr√©paration des donn√©es"}
            
            # 2. Entra√Ænement des mod√®les
            training_results = self.train_models(enabled_models)
            
            # 3. √âvaluation
            evaluation_results = self.evaluate_models(training_results)
            
            # 4. Rapport final
            final_report = self.generate_final_report(
                preparation_results, training_results, evaluation_results
            )
            
            total_time = time.time() - start_time
            
            logger.info("=" * 60)
            logger.info("üéâ PIPELINE COMPLET TERMIN√â AVEC SUCC√àS!")
            logger.info(f"‚è±Ô∏è Temps total: {total_time/60:.1f} minutes")
            logger.info(f"üìä Rapport final: {final_report}")
            
            return {
                "success": True,
                "preparation_results": preparation_results,
                "training_results": training_results,
                "evaluation_results": evaluation_results,
                "final_report": final_report,
                "total_time": total_time
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur dans le pipeline: {e}")
            return {"error": str(e)}

def main():
    """Fonction principale"""
    parser = argparse.ArgumentParser(description="FacturAI Fine-Tuning Orchestrator")
    parser.add_argument('--config', default='fine_tuning_config.json', 
                       help='Fichier de configuration')
    parser.add_argument('--mode', choices=['prepare', 'train', 'evaluate', 'all'], 
                       default='all', help='Mode d\'ex√©cution')
    parser.add_argument('--models', nargs='+', 
                       choices=['easyocr', 'trocr', 'paddleocr'],
                       help='Mod√®les √† entra√Æner')
    parser.add_argument('--check-only', action='store_true',
                       help='V√©rifier les pr√©requis uniquement')
    
    args = parser.parse_args()
    
    # Cr√©er l'orchestrateur
    orchestrator = FacturAIFineTuningOrchestrator(args.config)
    
    # Mode v√©rification uniquement
    if args.check_only:
        if orchestrator.check_requirements():
            print("‚úÖ Tous les pr√©requis sont satisfaits")
            sys.exit(0)
        else:
            print("‚ùå Probl√®mes d√©tect√©s")
            sys.exit(1)
    
    # Ex√©cution selon le mode
    if args.mode == 'prepare':
        results = orchestrator.prepare_data()
    elif args.mode == 'train':
        orchestrator.prepare_data()  # S'assurer que les donn√©es sont pr√™tes
        results = orchestrator.train_models(args.models)
    elif args.mode == 'evaluate':
        # Dummy training results pour l'√©valuation
        training_results = {}
        results = orchestrator.evaluate_models(training_results)
    else:  # mode 'all'
        results = orchestrator.run_complete_pipeline(args.models)
    
    # Affichage des r√©sultats
    if "error" in results:
        print(f"‚ùå Erreur: {results['error']}")
        sys.exit(1)
    elif "success" in results:
        print(f"\nüéâ Processus termin√© avec succ√®s!")
        if "final_report" in results:
            print(f"üìä Rapport final: {results['final_report']}")
    else:
        print("‚úÖ √âtape termin√©e")

if __name__ == "__main__":
    main()