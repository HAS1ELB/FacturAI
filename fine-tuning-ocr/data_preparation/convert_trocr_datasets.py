#!/usr/bin/env python3
"""
Script de conversion des datasets pour TrOCR
Convertit les datasets g√©n√©r√©s par data_preparation.py au format attendu par trocr_finetuning.py
"""

import json
import os
from pathlib import Path

def convert_datasets(fine_tuning_dir):
    """
    Convertit les datasets au format attendu par trocr_finetuning.py
    
    Args:
        fine_tuning_dir (str): Chemin vers le r√©pertoire contenant les donn√©es de fine-tuning
    """
    print("Conversion des datasets pour TrOCR...")
    
    # Chemins
    fine_tuning_dir = Path(fine_tuning_dir)
    splits_dir = fine_tuning_dir / "splits"
    trocr_dir = fine_tuning_dir / "datasets" / "trocr"
    annotations_dir = fine_tuning_dir / "annotations"
    
    # V√©rifier les r√©pertoires
    if not trocr_dir.exists() or not (trocr_dir / "dataset.json").exists():
        print(f"‚ùå Fichier TrOCR dataset.json non trouv√© dans {trocr_dir}")
        return False
    
    # S'assurer que le r√©pertoire annotations existe
    annotations_dir.mkdir(exist_ok=True)
    
    # Charger le dataset TrOCR
    with open(trocr_dir / "dataset.json", 'r', encoding='utf-8') as f:
        trocr_data = json.load(f)
    
    print(f"üìä Dataset TrOCR charg√©: {len(trocr_data)} √©chantillons")
    
    # Charger les splits
    splits = {}
    for split_name in ["train", "validation", "test"]:
        split_file = splits_dir / f"{split_name}.json"
        if split_file.exists():
            with open(split_file, 'r', encoding='utf-8') as f:
                splits[split_name] = json.load(f)
                print(f"üìä Split {split_name} charg√©: {len(splits[split_name])} √©chantillons")
    
    # Si pas de splits, cr√©er manuellement
    if not splits:
        print("‚ö†Ô∏è Aucun fichier de split trouv√©, cr√©ation manuelle...")
        # On prend 80% train, 10% validation, 10% test
        n = len(trocr_data)
        train_size = int(n * 0.8)
        val_size = int(n * 0.1)
        
        splits["train"] = trocr_data[:train_size]
        splits["validation"] = trocr_data[train_size:train_size+val_size]
        splits["test"] = trocr_data[train_size+val_size:]
    
    # Cr√©er les annotations pour chaque split
    for split_name, split_data in splits.items():
        # Filtrer les donn√©es TrOCR pour ce split
        annotations = []
        
        if isinstance(split_data, list):
            for item in split_data:
                image_path = item.get("image_path")
                
                # Trouver cet item dans le dataset TrOCR
                for trocr_item in trocr_data:
                    if trocr_item.get("image_path") == image_path:
                        annotations.append(trocr_item)
                        break
        
        # Si aucune annotation trouv√©e et nous avons cr√©√© les splits manuellement
        if not annotations and not all(os.path.exists(splits_dir / f"{s}.json") for s in ["train", "validation", "test"]):
            annotations = splits[split_name]
        
        # Sauvegarder les annotations
        output_file = annotations_dir / f"{split_name}_annotations.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(annotations, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ Annotations {split_name} sauvegard√©es: {len(annotations)} items")
    
    print("‚úÖ Conversion termin√©e!")
    return True

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Convertir les datasets pour TrOCR")
    parser.add_argument('--fine_tuning_dir', default='Data/fine_tuning',
                        help='R√©pertoire contenant les donn√©es de fine-tuning')
    
    args = parser.parse_args()
    convert_datasets(args.fine_tuning_dir)
