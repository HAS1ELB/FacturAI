#!/usr/bin/env python3
"""
Syst√®me complet de fine-tuning OCR pour factures
Supportant multiple approches : EasyOCR, TrOCR, PaddleOCR
"""

import os
import json
import logging
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
import numpy as np
import pandas as pd
from datetime import datetime

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OCRFineTuningManager:
    """Gestionnaire principal pour le fine-tuning OCR"""
    
    def __init__(self, config_path: str = "fine_tuning_config.json"):
        self.config = self.load_config(config_path)
        self.setup_directories()
        
    def load_config(self, config_path: str) -> Dict[str, Any]:
        """Charge la configuration du fine-tuning"""
        default_config = {
            "data": {
                "images_dir": "Data/processed_images",
                "annotations_dir": "Data/annotations",
                "output_dir": "Data/fine_tuning",
                "train_split": 0.8,
                "val_split": 0.1,
                "test_split": 0.1
            },
            "models": {
                "easyocr": {
                    "enabled": True,
                    "custom_model_path": "models/easyocr_finetuned",
                    "epochs": 50,
                    "batch_size": 8,
                    "learning_rate": 0.001
                },
                "trocr": {
                    "enabled": True,
                    "base_model": "microsoft/trocr-large-printed",
                    "output_dir": "models/trocr_finetuned",
                    "epochs": 30,
                    "batch_size": 4,
                    "learning_rate": 5e-5
                },
                "paddleocr": {
                    "enabled": True,
                    "output_dir": "models/paddleocr_finetuned",
                    "epochs": 100,
                    "batch_size": 16,
                    "learning_rate": 0.001
                }
            },
            "training": {
                "use_gpu": True,
                "mixed_precision": True,
                "gradient_accumulation_steps": 2,
                "warmup_steps": 1000,
                "max_length": 512,
                "early_stopping_patience": 5
            },
            "evaluation": {
                "metrics": ["accuracy", "edit_distance", "bleu", "character_error_rate"],
                "save_predictions": True,
                "generate_report": True
            }
        }
        
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
            # Merge configs
            default_config.update(user_config)
        else:
            # Sauvegarder la config par d√©faut
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, indent=2, ensure_ascii=False)
            logger.info(f"Configuration par d√©faut cr√©√©e: {config_path}")
        
        return default_config
    
    def setup_directories(self):
        """Cr√©e les dossiers n√©cessaires"""
        dirs_to_create = [
            self.config["data"]["annotations_dir"],
            self.config["data"]["output_dir"],
            "models/easyocr_finetuned",
            "models/trocr_finetuned", 
            "models/paddleocr_finetuned",
            "logs/training",
            "results/evaluation"
        ]
        
        for dir_path in dirs_to_create:
            os.makedirs(dir_path, exist_ok=True)
        
        logger.info("Dossiers de fine-tuning initialis√©s")
    
    def get_available_approaches(self) -> List[str]:
        """Retourne les approches de fine-tuning disponibles"""
        approaches = []
        for model_name, config in self.config["models"].items():
            if config.get("enabled", False):
                approaches.append(model_name)
        return approaches
    
    def run_full_pipeline(self, approach: str = "all"):
        """Ex√©cute le pipeline complet de fine-tuning"""
        logger.info("üöÄ D√âMARRAGE DU PIPELINE DE FINE-TUNING")
        logger.info("=" * 60)
        
        # 1. V√©rification des pr√©requis
        self.check_prerequisites()
        
        # 2. Pr√©paration des donn√©es
        self.prepare_dataset()
        
        # 3. Fine-tuning selon l'approche choisie
        if approach == "all":
            approaches = self.get_available_approaches()
        else:
            approaches = [approach] if approach in self.get_available_approaches() else []
        
        if not approaches:
            raise ValueError(f"Aucune approche valide trouv√©e. Disponibles: {self.get_available_approaches()}")
        
        results = {}
        for approach_name in approaches:
            logger.info(f"\nüéØ FINE-TUNING AVEC {approach_name.upper()}")
            logger.info("-" * 40)
            
            try:
                if approach_name == "easyocr":
                    results[approach_name] = self.finetune_easyocr()
                elif approach_name == "trocr":
                    results[approach_name] = self.finetune_trocr()
                elif approach_name == "paddleocr":
                    results[approach_name] = self.finetune_paddleocr()
                
                logger.info(f"‚úÖ {approach_name} fine-tuning termin√© avec succ√®s")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur lors du fine-tuning {approach_name}: {str(e)}")
                results[approach_name] = {"error": str(e)}
        
        # 4. √âvaluation comparative
        self.compare_models(results)
        
        # 5. G√©n√©ration du rapport final
        self.generate_final_report(results)
        
        logger.info("\nüéâ PIPELINE DE FINE-TUNING TERMIN√â")
        logger.info("=" * 60)
        
        return results
    
    def check_prerequisites(self):
        """V√©rifie que tous les pr√©requis sont install√©s"""
        logger.info("üìã V√©rification des pr√©requis...")
        
        required_packages = {
            "torch": "PyTorch pour l'entra√Ænement",
            "transformers": "Hugging Face Transformers pour TrOCR",
            "datasets": "Datasets pour la gestion des donn√©es",
            "easyocr": "EasyOCR pour le fine-tuning",
            "paddlepaddle": "PaddlePaddle pour PaddleOCR",
            "paddleocr": "PaddleOCR",
            "opencv-python": "OpenCV pour le traitement d'images",
            "Pillow": "PIL pour la manipulation d'images",
            "numpy": "NumPy pour les calculs num√©riques",
            "pandas": "Pandas pour la gestion des donn√©es",
            "scikit-learn": "Scikit-learn pour les m√©triques",
            "matplotlib": "Matplotlib pour la visualisation",
            "seaborn": "Seaborn pour les graphiques",
            "tqdm": "TQDM pour les barres de progression",
            "wandb": "Weights & Biases pour le monitoring (optionnel)"
        }
        
        missing_packages = []
        for package, description in required_packages.items():
            try:
                __import__(package.replace("-", "_"))
                logger.info(f"  ‚úÖ {package}: {description}")
            except ImportError:
                missing_packages.append(package)
                logger.warning(f"  ‚ùå {package}: {description} - MANQUANT")
        
        if missing_packages:
            logger.error(f"Packages manquants: {', '.join(missing_packages)}")
            logger.error("Installer avec: pip install " + " ".join(missing_packages))
            raise ImportError("Packages requis manquants")
        
        logger.info("‚úÖ Tous les pr√©requis sont install√©s")
    
    def prepare_dataset(self):
        """Pr√©pare le dataset pour l'entra√Ænement"""
        logger.info("üìä Pr√©paration du dataset...")
        
        # Cette m√©thode sera impl√©ment√©e dans le prochain fichier
        # car elle est complexe et n√©cessite plusieurs classes
        from data_preparation import DatasetPreparator
        
        preparator = DatasetPreparator(self.config)
        dataset_info = preparator.prepare_all_datasets()
        
        logger.info(f"‚úÖ Dataset pr√©par√©: {dataset_info}")
        return dataset_info
    
    def finetune_easyocr(self) -> Dict[str, Any]:
        """Fine-tuning d'EasyOCR"""
        logger.info("üîß Fine-tuning EasyOCR...")
        
        from easyocr_finetuning import EasyOCRFineTuner
        
        trainer = EasyOCRFineTuner(self.config["models"]["easyocr"])
        results = trainer.train()
        
        return results
    
    def finetune_trocr(self) -> Dict[str, Any]:
        """Fine-tuning de TrOCR"""
        logger.info("üîß Fine-tuning TrOCR...")
        
        from trocr_finetuning import TrOCRFineTuner
        
        trainer = TrOCRFineTuner(self.config["models"]["trocr"])
        results = trainer.train()
        
        return results
    
    def finetune_paddleocr(self) -> Dict[str, Any]:
        """Fine-tuning de PaddleOCR"""
        logger.info("üîß Fine-tuning PaddleOCR...")
        
        from paddleocr_finetuning import PaddleOCRFineTuner
        
        trainer = PaddleOCRFineTuner(self.config["models"]["paddleocr"])
        results = trainer.train()
        
        return results
    
    def compare_models(self, results: Dict[str, Any]):
        """Compare les performances des diff√©rents mod√®les"""
        logger.info("üìä Comparaison des mod√®les...")
        
        from model_evaluation import ModelComparator
        
        comparator = ModelComparator(self.config)
        comparison_report = comparator.compare_all_models(results)
        
        # Sauvegarder le rapport de comparaison
        report_path = "results/evaluation/model_comparison.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(comparison_report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"‚úÖ Rapport de comparaison sauvegard√©: {report_path}")
        
        return comparison_report
    
    def generate_final_report(self, results: Dict[str, Any]):
        """G√©n√®re le rapport final du fine-tuning"""
        logger.info("üìÑ G√©n√©ration du rapport final...")
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "config": self.config,
            "results": results,
            "summary": {
                "total_models_trained": len([r for r in results.values() if "error" not in r]),
                "failed_trainings": len([r for r in results.values() if "error" in r]),
                "best_model": self.find_best_model(results),
                "recommendations": self.generate_recommendations(results)
            }
        }
        
        # Sauvegarder le rapport
        report_path = f"results/evaluation/final_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"‚úÖ Rapport final sauvegard√©: {report_path}")
        
        # G√©n√©rer aussi un rapport HTML
        self.generate_html_report(report, report_path.replace('.json', '.html'))
        
        return report
    
    def find_best_model(self, results: Dict[str, Any]) -> str:
        """Trouve le meilleur mod√®le bas√© sur les m√©triques"""
        best_model = None
        best_score = 0
        
        for model_name, result in results.items():
            if "error" not in result and "evaluation" in result:
                # Score composite bas√© sur accuracy et faible taux d'erreur
                score = result["evaluation"].get("accuracy", 0) * 0.6 + \
                       (1 - result["evaluation"].get("character_error_rate", 1)) * 0.4
                
                if score > best_score:
                    best_score = score
                    best_model = model_name
        
        return best_model or "aucun"
    
    def generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """G√©n√®re des recommandations bas√©es sur les r√©sultats"""
        recommendations = []
        
        successful_models = [name for name, result in results.items() if "error" not in result]
        
        if not successful_models:
            recommendations.append("Aucun mod√®le n'a pu √™tre entra√Æn√© avec succ√®s. V√©rifiez les donn√©es et la configuration.")
            return recommendations
        
        best_model = self.find_best_model(results)
        if best_model != "aucun":
            recommendations.append(f"Utilisez le mod√®le {best_model} pour la production.")
        
        # Autres recommandations bas√©es sur les performances
        for model_name, result in results.items():
            if "error" not in result and "evaluation" in result:
                accuracy = result["evaluation"].get("accuracy", 0)
                if accuracy < 0.8:
                    recommendations.append(f"Le mod√®le {model_name} n√©cessite plus de donn√©es d'entra√Ænement (accuracy: {accuracy:.2%}).")
                elif accuracy > 0.95:
                    recommendations.append(f"Le mod√®le {model_name} montre d'excellentes performances (accuracy: {accuracy:.2%}).")
        
        recommendations.append("Consid√©rez l'ensemble des mod√®les pour cr√©er un syst√®me hybride.")
        recommendations.append("Collectez plus de donn√©es difficiles pour am√©liorer la robustesse.")
        
        return recommendations
    
    def generate_html_report(self, report: Dict[str, Any], output_path: str):
        """G√©n√®re un rapport HTML lisible"""
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Rapport de Fine-tuning OCR - FacturAI</title>
            <meta charset="utf-8">
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }}
                .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 10px; }}
                .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; text-align: center; border-radius: 10px; margin-bottom: 20px; }}
                .section {{ margin: 20px 0; padding: 15px; border-left: 4px solid #667eea; background: #f8f9fa; }}
                .metric {{ display: inline-block; margin: 10px; padding: 10px; background: white; border-radius: 5px; min-width: 120px; text-align: center; }}
                .success {{ color: #28a745; }}
                .warning {{ color: #ffc107; }}
                .error {{ color: #dc3545; }}
                .recommendation {{ background: #e7f3ff; padding: 10px; margin: 5px 0; border-radius: 5px; }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h1>ü§ñ Rapport de Fine-tuning OCR</h1>
                    <p>Projet FacturAI - {report['timestamp']}</p>
                </div>
                
                <div class="section">
                    <h2>üìä R√©sum√©</h2>
                    <div class="metric">
                        <h3>{report['summary']['total_models_trained']}</h3>
                        <p>Mod√®les entra√Æn√©s</p>
                    </div>
                    <div class="metric">
                        <h3 class="{'success' if report['summary']['failed_trainings'] == 0 else 'error'}">{report['summary']['failed_trainings']}</h3>
                        <p>√âchecs</p>
                    </div>
                    <div class="metric">
                        <h3 class="success">{report['summary']['best_model']}</h3>
                        <p>Meilleur mod√®le</p>
                    </div>
                </div>
                
                <div class="section">
                    <h2>üéØ Recommandations</h2>
                    {''.join([f'<div class="recommendation">üí° {rec}</div>' for rec in report['summary']['recommendations']])}
                </div>
                
                <div class="section">
                    <h2>üìà D√©tails des r√©sultats</h2>
                    <pre>{json.dumps(report['results'], indent=2, ensure_ascii=False)}</pre>
                </div>
            </div>
        </body>
        </html>
        """
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        logger.info(f"‚úÖ Rapport HTML g√©n√©r√©: {output_path}")

def main():
    """Fonction principale"""
    parser = argparse.ArgumentParser(description="Fine-tuning OCR pour FacturAI")
    parser.add_argument("--approach", choices=["easyocr", "trocr", "paddleocr", "all"], 
                       default="all", help="Approche de fine-tuning √† utiliser")
    parser.add_argument("--config", default="fine_tuning_config.json", 
                       help="Fichier de configuration")
    parser.add_argument("--prepare-only", action="store_true", 
                       help="Pr√©parer seulement les donn√©es, sans entra√Ænement")
    
    args = parser.parse_args()
    
    # Initialiser le gestionnaire
    manager = OCRFineTuningManager(args.config)
    
    if args.prepare_only:
        logger.info("Mode pr√©paration uniquement")
        manager.prepare_dataset()
    else:
        # Ex√©cuter le pipeline complet
        results = manager.run_full_pipeline(args.approach)
        
        # Afficher un r√©sum√©
        print("\n" + "="*60)
        print("üéâ FINE-TUNING TERMIN√â")
        print("="*60)
        
        for model_name, result in results.items():
            if "error" in result:
                print(f"‚ùå {model_name}: {result['error']}")
            else:
                accuracy = result.get("evaluation", {}).get("accuracy", 0)
                print(f"‚úÖ {model_name}: {accuracy:.2%} accuracy")
        
        best_model = manager.find_best_model(results)
        print(f"\nüèÜ Meilleur mod√®le: {best_model}")
        print(f"üìÅ R√©sultats dans: results/evaluation/")

if __name__ == "__main__":
    main()